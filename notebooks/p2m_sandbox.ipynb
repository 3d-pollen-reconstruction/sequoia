{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel 2 Mesh Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1804.01654v2\n",
    "# p2m model init needs camera focal length, camera center and mesh center\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def batch_mm(matrix, batch):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/pytorch/issues/14489\n",
    "    \"\"\"\n",
    "    # TODO: accelerate this with batch operations\n",
    "    return torch.stack([matrix.mm(b) for b in batch], dim=0)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for torch.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        return batch_mm(x, y)\n",
    "    else:\n",
    "        return torch.matmul(x, y)\n",
    "\n",
    "class GConv(nn.Module):\n",
    "    \"\"\"Simple GCN layer\n",
    "\n",
    "    Similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, adj_mat, bias=True):\n",
    "        super(GConv, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.adj_mat = nn.Parameter(adj_mat, requires_grad=False)\n",
    "        self.weight = nn.Parameter(torch.zeros((in_features, out_features), dtype=torch.float))\n",
    "        # Following https://github.com/Tong-ZHAO/Pixel2Mesh-Pytorch/blob/a0ae88c4a42eef6f8f253417b97df978db842708/model/gcn_layers.py#L45\n",
    "        # This seems to be different from the original implementation of P2M\n",
    "        self.loop_weight = nn.Parameter(torch.zeros((in_features, out_features), dtype=torch.float))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((out_features,), dtype=torch.float))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.xavier_uniform_(self.loop_weight.data)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        support = torch.matmul(inputs, self.weight)\n",
    "        support_loop = torch.matmul(inputs, self.loop_weight)\n",
    "        output = dot(self.adj_mat, support, True) + support_loop\n",
    "        if self.bias is not None:\n",
    "            ret = output + self.bias\n",
    "        else:\n",
    "            ret = output\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GConv(nn.Module):\n",
    "    \"\"\"Simple GCN layer\n",
    "\n",
    "    Similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, adj_mat, bias=True):\n",
    "        super(GConv, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.adj_mat = nn.Parameter(adj_mat, requires_grad=False)\n",
    "        self.weight = nn.Parameter(torch.zeros((in_features, out_features), dtype=torch.float))\n",
    "        # Following https://github.com/Tong-ZHAO/Pixel2Mesh-Pytorch/blob/a0ae88c4a42eef6f8f253417b97df978db842708/model/gcn_layers.py#L45\n",
    "        # This seems to be different from the original implementation of P2M\n",
    "        self.loop_weight = nn.Parameter(torch.zeros((in_features, out_features), dtype=torch.float))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((out_features,), dtype=torch.float))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.xavier_uniform_(self.loop_weight.data)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        support = torch.matmul(inputs, self.weight)\n",
    "        support_loop = torch.matmul(inputs, self.loop_weight)\n",
    "        output = dot(self.adj_mat, support, True) + support_loop\n",
    "        if self.bias is not None:\n",
    "            ret = output + self.bias\n",
    "        else:\n",
    "            ret = output\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "class GResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, hidden_dim, adj_mat, activation=None):\n",
    "        super(GResBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = GConv(in_features=in_dim, out_features=hidden_dim, adj_mat=adj_mat)\n",
    "        self.conv2 = GConv(in_features=hidden_dim, out_features=in_dim, adj_mat=adj_mat)\n",
    "        self.activation = F.relu if activation else None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return (inputs + x) * 0.5\n",
    "\n",
    "\n",
    "class GBottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self, block_num, in_dim, hidden_dim, out_dim, adj_mat, activation=None):\n",
    "        super(GBottleneck, self).__init__()\n",
    "\n",
    "        resblock_layers = [GResBlock(in_dim=hidden_dim, hidden_dim=hidden_dim, adj_mat=adj_mat, activation=activation)\n",
    "                           for _ in range(block_num)]\n",
    "        self.blocks = nn.Sequential(*resblock_layers)\n",
    "        self.conv1 = GConv(in_features=in_dim, out_features=hidden_dim, adj_mat=adj_mat)\n",
    "        self.conv2 = GConv(in_features=hidden_dim, out_features=out_dim, adj_mat=adj_mat)\n",
    "        self.activation = F.relu if activation else None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        x_hidden = self.blocks(x)\n",
    "        x_out = self.conv2(x_hidden)\n",
    "\n",
    "        return x_out, x_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import ResNet\n",
    "from torchvision.models.resnet import Bottleneck\n",
    "\n",
    "PRETRAINED_WEIGHTS_PATH = 'path/to/pretrained/weights'\n",
    "\n",
    "class P2MResNet(ResNet):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.output_dim = 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        res = super()._make_layer(block, planes, blocks, stride=stride, dilate=dilate)\n",
    "        self.output_dim += self.inplanes\n",
    "        return res\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        features = []\n",
    "        x = self.layer1(x)\n",
    "        features.append(x)\n",
    "        x = self.layer2(x)\n",
    "        features.append(x)\n",
    "        x = self.layer3(x)\n",
    "        features.append(x)\n",
    "        x = self.layer4(x)\n",
    "        features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "    @property\n",
    "    def features_dim(self):\n",
    "        return self.output_dim\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    model = P2MResNet(Bottleneck, [3, 4, 6, 3])\n",
    "    state_dict = torch.load(PRETRAINED_WEIGHTS_PATH)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16TensorflowAlign(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes_input=3):\n",
    "        super(VGG16TensorflowAlign, self).__init__()\n",
    "\n",
    "        self.features_dim = 960\n",
    "        # this is to align with tensorflow padding (with stride)\n",
    "        # https://bugxch.github.io/tf%E4%B8%AD%E7%9A%84padding%E6%96%B9%E5%BC%8FSAME%E5%92%8CVALID%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/\n",
    "        self.same_padding = nn.ZeroPad2d(1)\n",
    "        self.tf_padding = nn.ZeroPad2d((0, 1, 0, 1))\n",
    "        self.tf_padding_2 = nn.ZeroPad2d((1, 2, 1, 2))\n",
    "\n",
    "        self.conv0_1 = nn.Conv2d(n_classes_input, 16, 3, stride=1, padding=0)\n",
    "        self.conv0_2 = nn.Conv2d(16, 16, 3, stride=1, padding=0)\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  # 224 -> 112\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "        self.conv1_3 = nn.Conv2d(32, 32, 3, stride=1, padding=0)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, 3, stride=2, padding=0)  # 112 -> 56\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
    "        self.conv2_3 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, 3, stride=2, padding=0)  # 56 -> 28\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=0)\n",
    "        self.conv3_3 = nn.Conv2d(128, 128, 3, stride=1, padding=0)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, 5, stride=2, padding=0)  # 28 -> 14\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, 3, stride=1, padding=0)\n",
    "        self.conv4_3 = nn.Conv2d(256, 256, 3, stride=1, padding=0)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, 5, stride=2, padding=0)  # 14 -> 7\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=0)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=0)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, 3, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = F.relu(self.conv0_1(self.same_padding(img)))\n",
    "        img = F.relu(self.conv0_2(self.same_padding(img)))\n",
    "\n",
    "        img = F.relu(self.conv1_1(self.tf_padding(img)))\n",
    "        img = F.relu(self.conv1_2(self.same_padding(img)))\n",
    "        img = F.relu(self.conv1_3(self.same_padding(img)))\n",
    "\n",
    "        img = F.relu(self.conv2_1(self.tf_padding(img)))\n",
    "        img = F.relu(self.conv2_2(self.same_padding(img)))\n",
    "        img = F.relu(self.conv2_3(self.same_padding(img)))\n",
    "        img2 = img\n",
    "\n",
    "        img = F.relu(self.conv3_1(self.tf_padding(img)))\n",
    "        img = F.relu(self.conv3_2(self.same_padding(img)))\n",
    "        img = F.relu(self.conv3_3(self.same_padding(img)))\n",
    "        img3 = img\n",
    "\n",
    "        img = F.relu(self.conv4_1(self.tf_padding_2(img)))\n",
    "        img = F.relu(self.conv4_2(self.same_padding(img)))\n",
    "        img = F.relu(self.conv4_3(self.same_padding(img)))\n",
    "        img4 = img\n",
    "\n",
    "        img = F.relu(self.conv5_1(self.tf_padding_2(img)))\n",
    "        img = F.relu(self.conv5_2(self.same_padding(img)))\n",
    "        img = F.relu(self.conv5_3(self.same_padding(img)))\n",
    "        img = F.relu(self.conv5_4(self.same_padding(img)))\n",
    "        img5 = img\n",
    "\n",
    "        return [img2, img3, img4, img5]\n",
    "\n",
    "\n",
    "class VGG16P2M(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes_input=3, pretrained=False):\n",
    "        super(VGG16P2M, self).__init__()\n",
    "\n",
    "        self.features_dim = 960\n",
    "\n",
    "        self.conv0_1 = nn.Conv2d(n_classes_input, 16, 3, stride=1, padding=1)\n",
    "        self.conv0_2 = nn.Conv2d(16, 16, 3, stride=1, padding=1)\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(16, 32, 3, stride=2, padding=1)  # 224 -> 112\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "        self.conv1_3 = nn.Conv2d(32, 32, 3, stride=1, padding=1)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, 3, stride=2, padding=1)  # 112 -> 56\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.conv2_3 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, 3, stride=2, padding=1)  # 56 -> 28\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(128, 256, 5, stride=2, padding=2)  # 28 -> 14\n",
    "        self.conv4_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(256, 512, 5, stride=2, padding=2)  # 14 -> 7\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "\n",
    "        if \"vgg16p2m\" in PRETRAINED_WEIGHTS_PATH and pretrained:\n",
    "            state_dict = torch.load(PRETRAINED_WEIGHTS_PATH)\n",
    "            self.load_state_dict(state_dict)\n",
    "        else:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, img):\n",
    "        img = F.relu(self.conv0_1(img))\n",
    "        img = F.relu(self.conv0_2(img))\n",
    "        # img0 = torch.squeeze(img) # 224\n",
    "\n",
    "        img = F.relu(self.conv1_1(img))\n",
    "        img = F.relu(self.conv1_2(img))\n",
    "        img = F.relu(self.conv1_3(img))\n",
    "        # img1 = torch.squeeze(img) # 112\n",
    "\n",
    "        img = F.relu(self.conv2_1(img))\n",
    "        img = F.relu(self.conv2_2(img))\n",
    "        img = F.relu(self.conv2_3(img))\n",
    "        img2 = img\n",
    "\n",
    "        img = F.relu(self.conv3_1(img))\n",
    "        img = F.relu(self.conv3_2(img))\n",
    "        img = F.relu(self.conv3_3(img))\n",
    "        img3 = img\n",
    "\n",
    "        img = F.relu(self.conv4_1(img))\n",
    "        img = F.relu(self.conv4_2(img))\n",
    "        img = F.relu(self.conv4_3(img))\n",
    "        img4 = img\n",
    "\n",
    "        img = F.relu(self.conv5_1(img))\n",
    "        img = F.relu(self.conv5_2(img))\n",
    "        img = F.relu(self.conv5_3(img))\n",
    "        img = F.relu(self.conv5_4(img))\n",
    "        img5 = img\n",
    "\n",
    "        return [img2, img3, img4, img5]\n",
    "\n",
    "\n",
    "class VGG16Recons(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, image_channel=3):\n",
    "        super(VGG16Recons, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.ConvTranspose2d(input_dim, 256, kernel_size=2, stride=2, padding=0)  # 7 -> 14\n",
    "        self.conv_2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)  # 14 -> 28\n",
    "        self.conv_3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)  # 28 -> 56\n",
    "        self.conv_4 = nn.ConvTranspose2d(128, 32, kernel_size=6, stride=2, padding=2)  # 56 -> 112\n",
    "        self.conv_5 = nn.ConvTranspose2d(32, image_channel, kernel_size=6, stride=2, padding=2)  # 112 -> 224\n",
    "\n",
    "    def forward(self, img_feats):\n",
    "        x = F.relu(self.conv_1(img_feats[-1]))\n",
    "        x = torch.cat((x, img_feats[-2]), dim=1)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = torch.cat((x, img_feats[-3]), dim=1)\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = torch.cat((x, img_feats[-4]), dim=1)\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = F.relu(self.conv_5(x))\n",
    "\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(options):\n",
    "    if options.backbone.startswith(\"vgg16\"):\n",
    "        if options.align_with_tensorflow:\n",
    "            nn_encoder = VGG16TensorflowAlign()\n",
    "        else:\n",
    "            nn_encoder = VGG16P2M(pretrained=\"pretrained\" in options.backbone)\n",
    "        nn_decoder = VGG16Recons()\n",
    "    elif options.backbone == \"resnet50\":\n",
    "        nn_encoder = resnet50()\n",
    "        nn_decoder = None\n",
    "    else:\n",
    "        raise NotImplementedError(\"No implemented backbone called '%s' found\" % options.backbone)\n",
    "    return nn_encoder, nn_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GUnpooling(nn.Module):\n",
    "    \"\"\"Graph Pooling layer, aims to add additional vertices to the graph.\n",
    "    The middle point of each edges are added, and its feature is simply\n",
    "    the average of the two edge vertices.\n",
    "    Three middle points are connected in each triangle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unpool_idx):\n",
    "        super(GUnpooling, self).__init__()\n",
    "        self.unpool_idx = unpool_idx\n",
    "        # save dim info\n",
    "        self.in_num = torch.max(unpool_idx).item()\n",
    "        self.out_num = self.in_num + len(unpool_idx)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        new_features = inputs[:, self.unpool_idx].clone()\n",
    "        new_vertices = 0.5 * new_features.sum(2)\n",
    "        output = torch.cat([inputs, new_vertices], 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_num) + ' -> ' \\\n",
    "               + str(self.out_num) + ')'\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn import Threshold\n",
    "\n",
    "class GProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Projection layer, which pool 2D features to mesh\n",
    "\n",
    "    The layer projects a vertex of the mesh to the 2D image and use\n",
    "    bi-linear interpolation to get the corresponding feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mesh_pos, camera_f, camera_c, bound=0, tensorflow_compatible=False):\n",
    "        super(GProjection, self).__init__()\n",
    "        self.mesh_pos, self.camera_f, self.camera_c = mesh_pos, camera_f, camera_c\n",
    "        self.threshold = None\n",
    "        self.bound = 0\n",
    "        self.tensorflow_compatible = tensorflow_compatible\n",
    "        if self.bound != 0:\n",
    "            self.threshold = Threshold(bound, bound)\n",
    "\n",
    "    def bound_val(self, x):\n",
    "        \"\"\"\n",
    "        given x, return min(threshold, x), in case threshold is not None\n",
    "        \"\"\"\n",
    "        if self.bound < 0:\n",
    "            return -self.threshold(-x)\n",
    "        elif self.bound > 0:\n",
    "            return self.threshold(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def image_feature_shape(img):\n",
    "        return np.array([img.size(-1), img.size(-2)])\n",
    "\n",
    "    def project_tensorflow(self, x, y, img_size, img_feat):\n",
    "        x = torch.clamp(x, min=0, max=img_size[1] - 1)\n",
    "        y = torch.clamp(y, min=0, max=img_size[0] - 1)\n",
    "\n",
    "        # it's tedious and contains bugs...\n",
    "        # when x1 = x2, the area is 0, therefore it won't be processed\n",
    "        # keep it here to align with tensorflow version\n",
    "        x1, x2 = torch.floor(x).long(), torch.ceil(x).long()\n",
    "        y1, y2 = torch.floor(y).long(), torch.ceil(y).long()\n",
    "\n",
    "        Q11 = img_feat[:, x1, y1].clone()\n",
    "        Q12 = img_feat[:, x1, y2].clone()\n",
    "        Q21 = img_feat[:, x2, y1].clone()\n",
    "        Q22 = img_feat[:, x2, y2].clone()\n",
    "\n",
    "        weights = torch.mul(x2.float() - x, y2.float() - y)\n",
    "        Q11 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q11, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x2.float() - x, y - y1.float())\n",
    "        Q12 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q12, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x - x1.float(), y2.float() - y)\n",
    "        Q21 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q21, 0, 1))\n",
    "\n",
    "        weights = torch.mul(x - x1.float(), y - y1.float())\n",
    "        Q22 = torch.mul(weights.unsqueeze(-1), torch.transpose(Q22, 0, 1))\n",
    "\n",
    "        output = Q11 + Q21 + Q12 + Q22\n",
    "        return output\n",
    "\n",
    "    def forward(self, resolution, img_features, inputs):\n",
    "        half_resolution = (resolution - 1) / 2\n",
    "        camera_c_offset = np.array(self.camera_c) - half_resolution\n",
    "        # map to [-1, 1]\n",
    "        # not sure why they render to negative x\n",
    "        positions = inputs + torch.tensor(self.mesh_pos, device=inputs.device, dtype=torch.float)\n",
    "        w = -self.camera_f[0] * (positions[:, :, 0] / self.bound_val(positions[:, :, 2])) + camera_c_offset[0]\n",
    "        h = self.camera_f[1] * (positions[:, :, 1] / self.bound_val(positions[:, :, 2])) + camera_c_offset[1]\n",
    "\n",
    "        if self.tensorflow_compatible:\n",
    "            # to align with tensorflow\n",
    "            # this is incorrect, I believe\n",
    "            w += half_resolution[0]\n",
    "            h += half_resolution[1]\n",
    "\n",
    "        else:\n",
    "            # directly do clamping\n",
    "            w /= half_resolution[0]\n",
    "            h /= half_resolution[1]\n",
    "\n",
    "            # clamp to [-1, 1]\n",
    "            w = torch.clamp(w, min=-1, max=1)\n",
    "            h = torch.clamp(h, min=-1, max=1)\n",
    "\n",
    "        feats = [inputs]\n",
    "        for img_feature in img_features:\n",
    "            feats.append(self.project(resolution, img_feature, torch.stack([w, h], dim=-1)))\n",
    "\n",
    "        output = torch.cat(feats, 2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def project(self, img_shape, img_feat, sample_points):\n",
    "        \"\"\"\n",
    "        :param img_shape: raw image shape\n",
    "        :param img_feat: [batch_size x channel x h x w]\n",
    "        :param sample_points: [batch_size x num_points x 2], in range [-1, 1]\n",
    "        :return: [batch_size x num_points x feat_dim]\n",
    "        \"\"\"\n",
    "        if self.tensorflow_compatible:\n",
    "            feature_shape = self.image_feature_shape(img_feat)\n",
    "            points_w = sample_points[:, :, 0] / (img_shape[0] / feature_shape[0])\n",
    "            points_h = sample_points[:, :, 1] / (img_shape[1] / feature_shape[1])\n",
    "            output = torch.stack([self.project_tensorflow(points_h[i], points_w[i],\n",
    "                                                          feature_shape, img_feat[i]) for i in range(img_feat.size(0))], 0)\n",
    "        else:\n",
    "            output = F.grid_sample(img_feat, sample_points.unsqueeze(1))\n",
    "            output = torch.transpose(output.squeeze(2), 1, 2)\n",
    "\n",
    "        return output\n",
    "\n",
    "class P2MModel(nn.Module):\n",
    "\n",
    "    def __init__(self, options, ellipsoid, camera_f, camera_c, mesh_pos):\n",
    "        super(P2MModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = options.hidden_dim\n",
    "        self.coord_dim = options.coord_dim\n",
    "        self.last_hidden_dim = options.last_hidden_dim\n",
    "        self.init_pts = nn.Parameter(ellipsoid.coord, requires_grad=False)\n",
    "        self.gconv_activation = options.gconv_activation\n",
    "\n",
    "        self.nn_encoder, self.nn_decoder = get_backbone(options)\n",
    "        self.features_dim = self.nn_encoder.features_dim + self.coord_dim\n",
    "\n",
    "        self.gcns = nn.ModuleList([\n",
    "            GBottleneck(6, self.features_dim, self.hidden_dim, self.coord_dim,\n",
    "                        ellipsoid.adj_mat[0], activation=self.gconv_activation),\n",
    "            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.coord_dim,\n",
    "                        ellipsoid.adj_mat[1], activation=self.gconv_activation),\n",
    "            GBottleneck(6, self.features_dim + self.hidden_dim, self.hidden_dim, self.last_hidden_dim,\n",
    "                        ellipsoid.adj_mat[2], activation=self.gconv_activation)\n",
    "        ])\n",
    "\n",
    "        self.unpooling = nn.ModuleList([\n",
    "            GUnpooling(ellipsoid.unpool_idx[0]),\n",
    "            GUnpooling(ellipsoid.unpool_idx[1])\n",
    "        ])\n",
    "\n",
    "        # if options.align_with_tensorflow:\n",
    "        #     self.projection = GProjection\n",
    "        # else:\n",
    "        #     self.projection = GProjection\n",
    "        self.projection = GProjection(mesh_pos, camera_f, camera_c, bound=options.z_threshold,\n",
    "                                      tensorflow_compatible=options.align_with_tensorflow)\n",
    "\n",
    "        self.gconv = GConv(in_features=self.last_hidden_dim, out_features=self.coord_dim,\n",
    "                           adj_mat=ellipsoid.adj_mat[2])\n",
    "\n",
    "    def forward(self, img):\n",
    "        batch_size = img.size(0)\n",
    "        img_feats = self.nn_encoder(img)\n",
    "        img_shape = self.projection.image_feature_shape(img)\n",
    "\n",
    "        init_pts = self.init_pts.data.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        # GCN Block 1\n",
    "        x = self.projection(img_shape, img_feats, init_pts)\n",
    "        x1, x_hidden = self.gcns[0](x)\n",
    "\n",
    "        # before deformation 2\n",
    "        x1_up = self.unpooling[0](x1)\n",
    "\n",
    "        # GCN Block 2\n",
    "        x = self.projection(img_shape, img_feats, x1)\n",
    "        x = self.unpooling[0](torch.cat([x, x_hidden], 2))\n",
    "        # after deformation 2\n",
    "        x2, x_hidden = self.gcns[1](x)\n",
    "\n",
    "        # before deformation 3\n",
    "        x2_up = self.unpooling[1](x2)\n",
    "\n",
    "        # GCN Block 3\n",
    "        x = self.projection(img_shape, img_feats, x2)\n",
    "        x = self.unpooling[1](torch.cat([x, x_hidden], 2))\n",
    "        x3, _ = self.gcns[2](x)\n",
    "        if self.gconv_activation:\n",
    "            x3 = F.relu(x3)\n",
    "        # after deformation 3\n",
    "        x3 = self.gconv(x3)\n",
    "\n",
    "        if self.nn_decoder is not None:\n",
    "            reconst = self.nn_decoder(img_feats)\n",
    "        else:\n",
    "            reconst = None\n",
    "\n",
    "        return {\n",
    "            \"pred_coord\": [x1, x2, x3],\n",
    "            \"pred_coord_before_deform\": [init_pts, x1_up, x2_up],\n",
    "            \"reconst\": reconst\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "ELLIPSOID_PATH = '.'\n",
    "\n",
    "def torch_sparse_tensor(indices, value, size):\n",
    "    coo = coo_matrix((value, (indices[:, 0], indices[:, 1])), shape=size)\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "    i = torch.tensor(indices, dtype=torch.long)\n",
    "    v = torch.tensor(values, dtype=torch.float)\n",
    "    shape = coo.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, shape)\n",
    "\n",
    "\n",
    "class Ellipsoid(object):\n",
    "\n",
    "    def __init__(self, mesh_pos, file=ELLIPSOID_PATH):\n",
    "        with open(file, \"rb\") as fp:\n",
    "            fp_info = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "        # shape: n_pts * 3\n",
    "        self.coord = torch.tensor(fp_info[0]) - torch.tensor(mesh_pos, dtype=torch.float)\n",
    "\n",
    "        # edges & faces & lap_idx\n",
    "        # edge: num_edges * 2\n",
    "        # faces: num_faces * 4\n",
    "        # laplace_idx: num_pts * 10\n",
    "        self.edges, self.laplace_idx = [], []\n",
    "\n",
    "        for i in range(3):\n",
    "            self.edges.append(torch.tensor(fp_info[1 + i][1][0], dtype=torch.long))\n",
    "            self.laplace_idx.append(torch.tensor(fp_info[7][i], dtype=torch.long))\n",
    "\n",
    "        # unpool index\n",
    "        # num_pool_edges * 2\n",
    "        # pool_01: 462 * 2, pool_12: 1848 * 2\n",
    "        self.unpool_idx = [torch.tensor(fp_info[4][i], dtype=torch.long) for i in range(2)]\n",
    "\n",
    "        # loops and adjacent edges\n",
    "        self.adj_mat = []\n",
    "        for i in range(1, 4):\n",
    "            # 0: np.array, 2D, pos\n",
    "            # 1: np.array, 1D, vals\n",
    "            # 2: tuple - shape, n * n\n",
    "            adj_mat = torch_sparse_tensor(*fp_info[i][1])\n",
    "            self.adj_mat.append(adj_mat)\n",
    "\n",
    "        ellipsoid_dir = os.path.dirname(file)\n",
    "        self.faces = []\n",
    "        self.obj_fmt_faces = []\n",
    "        # faces: f * 3, original ellipsoid, and two after deformations\n",
    "        for i in range(1, 4):\n",
    "            face_file = os.path.join(ellipsoid_dir, \"face%d.obj\" % i)\n",
    "            faces = np.loadtxt(face_file, dtype='|S32')\n",
    "            self.obj_fmt_faces.append(faces)\n",
    "            self.faces.append(torch.tensor(faces[:, 1:].astype(np.int) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. Choose the sphere topology and resolution\n",
    "#    - icosphere gives more uniform triangles\n",
    "#    - uv_sphere gives lat/long layout if you prefer\n",
    "sphere = trimesh.creation.icosphere(subdivisions=4, radius=1.0)\n",
    "# sphere = trimesh.creation.uv_sphere(radius=1.0, count=[64, 64])\n",
    "\n",
    "# 2. Define your ellipsoid semi-axes\n",
    "a, b, c = 1.0, 1.2, 0.8  # for example\n",
    "\n",
    "# 3. Create and apply the scale\n",
    "scale_mat = np.diag([a, b, c, 1.0])\n",
    "ellipsoid = sphere.copy()\n",
    "ellipsoid.apply_transform(scale_mat)\n",
    "\n",
    "# 4. Export to PyTorch (optional)\n",
    "verts = torch.from_numpy(ellipsoid.vertices).float()     # (V, 3)\n",
    "faces = torch.from_numpy(ellipsoid.faces.astype(np.int64)).long()  # (F, 3)\n",
    "\n",
    "ellipsoid.show()\n",
    "\n",
    "# save the ellipsoid to a file\n",
    "with open('ellipsoid.pkl', 'wb') as f:\n",
    "    pickle.dump((verts, faces), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2562, 3]), torch.Size([5120, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts.shape, faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "from argparse import ArgumentParser\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "options = edict()\n",
    "\n",
    "options.name = 'p2m'\n",
    "options.version = None\n",
    "options.num_workers = 1\n",
    "options.num_gpus = 1\n",
    "options.pin_memory = True\n",
    "\n",
    "options.log_dir = \"logs\"\n",
    "options.log_level = \"info\"\n",
    "options.summary_dir = \"summary\"\n",
    "options.checkpoint_dir = \"checkpoints\"\n",
    "options.checkpoint = None\n",
    "\n",
    "options.dataset = edict()\n",
    "options.dataset.name = \"shapenet\"\n",
    "options.dataset.subset_train = \"train_small\"\n",
    "options.dataset.subset_eval = \"test_small\"\n",
    "options.dataset.camera_f = [248., 248.]\n",
    "options.dataset.camera_c = [111.5, 111.5]\n",
    "options.dataset.mesh_pos = [0., 0., -0.8]\n",
    "options.dataset.normalization = True\n",
    "options.dataset.num_classes = 13\n",
    "\n",
    "options.dataset.shapenet = edict()\n",
    "options.dataset.shapenet.num_points = 3000\n",
    "options.dataset.shapenet.resize_with_constant_border = False\n",
    "\n",
    "options.dataset.predict = edict()\n",
    "options.dataset.predict.folder = \"/tmp\"\n",
    "\n",
    "options.model = edict()\n",
    "options.model.name = \"pixel2mesh\"\n",
    "options.model.hidden_dim = 192\n",
    "options.model.last_hidden_dim = 192\n",
    "options.model.coord_dim = 3\n",
    "options.model.backbone = \"vgg16\"\n",
    "options.model.gconv_activation = True\n",
    "# provide a boundary for z, so that z will never be equal to 0, on denominator\n",
    "# if z is greater than 0, it will never be less than z;\n",
    "# if z is less than 0, it will never be greater than z.\n",
    "options.model.z_threshold = 0\n",
    "# align with original tensorflow model\n",
    "# please follow experiments/tensorflow.yml\n",
    "options.model.align_with_tensorflow = False\n",
    "\n",
    "options.loss = edict()\n",
    "options.loss.weights = edict()\n",
    "options.loss.weights.normal = 1.6e-4\n",
    "options.loss.weights.edge = 0.3\n",
    "options.loss.weights.laplace = 0.5\n",
    "options.loss.weights.move = 0.1\n",
    "options.loss.weights.constant = 1.\n",
    "options.loss.weights.chamfer = [1., 1., 1.]\n",
    "options.loss.weights.chamfer_opposite = 1.\n",
    "options.loss.weights.reconst = 0.\n",
    "\n",
    "options.train = edict()\n",
    "options.train.num_epochs = 50\n",
    "options.train.batch_size = 4\n",
    "options.train.summary_steps = 50\n",
    "options.train.checkpoint_steps = 10000\n",
    "options.train.test_epochs = 1\n",
    "options.train.use_augmentation = True\n",
    "options.train.shuffle = True\n",
    "\n",
    "options.test = edict()\n",
    "options.test.dataset = []\n",
    "options.test.summary_steps = 50\n",
    "options.test.batch_size = 4\n",
    "options.test.shuffle = False\n",
    "options.test.weighted_mean = False\n",
    "\n",
    "options.optim = edict()\n",
    "options.optim.name = \"adam\"\n",
    "options.optim.adam_beta1 = 0.9\n",
    "options.optim.sgd_momentum = 0.9\n",
    "options.optim.lr = 5.0E-5\n",
    "options.optim.wd = 1.0E-6\n",
    "options.optim.lr_step = [30, 45]\n",
    "options.optim.lr_factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahrn\\AppData\\Local\\Temp\\ipykernel_11728\\3870128757.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.coord = torch.tensor(fp_info[0]) - torch.tensor(mesh_pos, dtype=torch.float)\n",
      "C:\\Users\\fahrn\\AppData\\Local\\Temp\\ipykernel_11728\\3870128757.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edges.append(torch.tensor(fp_info[1 + i][1][0], dtype=torch.long))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ellipsoid = \u001b[43mEllipsoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mellipsoid.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mEllipsoid.__init__\u001b[39m\u001b[34m(self, mesh_pos, file)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m.edges.append(torch.tensor(fp_info[\u001b[32m1\u001b[39m + i][\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m], dtype=torch.long))\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28mself\u001b[39m.laplace_idx.append(torch.tensor(\u001b[43mfp_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m]\u001b[49m[i], dtype=torch.long))\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# unpool index\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# num_pool_edges * 2\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# pool_01: 462 * 2, pool_12: 1848 * 2\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.unpool_idx = [torch.tensor(fp_info[\u001b[32m4\u001b[39m][i], dtype=torch.long) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m)]\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "ellipsoid = Ellipsoid(mesh_pos=torch.zeros(3), file='ellipsoid.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = P2MModel(\n",
    "    options=options,\n",
    "    ellipsoid=ellipsoid,\n",
    "    camera_f=camera_f,\n",
    "    camera_c=camera_c,\n",
    "    mesh_pos=mesh_pos\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
