{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/super/Documents/GitHub/sequoia/data\\\\processed\\\\images'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 314\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDevice:\u001b[39m\u001b[33m\"\u001b[39m, dev)\n\u001b[32m    313\u001b[39m tf = transforms.ToTensor()\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m dataset, train_ids, _ = \u001b[43mget_train_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_transforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m (l_img, r_img), pts, rot, vox = dataset[train_ids[\u001b[32m0\u001b[39m]]\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# prepare two views as before\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\super\\Documents\\Github\\sequoia\\pipeline\\..\\data\\pollen_dataset.py:80\u001b[39m, in \u001b[36mget_train_test_split\u001b[39m\u001b[34m(test_ratio, seed, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_train_test_split\u001b[39m(test_ratio=\u001b[32m0.2\u001b[39m, seed=\u001b[32m42\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     dataset = \u001b[43mPollenDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     train_ids, test_ids = train_test_split(\n\u001b[32m     82\u001b[39m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))), test_size=test_ratio, random_state=seed\n\u001b[32m     83\u001b[39m     )\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset, train_ids, test_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\super\\Documents\\Github\\sequoia\\pipeline\\..\\data\\pollen_dataset.py:36\u001b[39m, in \u001b[36mPollenDataset.__init__\u001b[39m\u001b[34m(self, image_transforms, mesh_transforms, device)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m.mesh_transform   = mesh_transforms  \u001b[38;5;66;03m# if you want per‑mesh augmentations\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.device           = device\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28mself\u001b[39m.image_files = \u001b[38;5;28msorted\u001b[39m(\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimages_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.rotations   = pd.read_csv(\u001b[38;5;28mself\u001b[39m.rotations_csv)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\super\\Documents\\Github\\sequoia\\pipeline\\..\\data\\utils.py:8\u001b[39m, in \u001b[36mlist_files\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_files\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) -> Iterable[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return names of regular files (no sub‑dirs) in *path*.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [entry.name \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry.is_file()]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'C:/Users/super/Documents/GitHub/sequoia/data\\\\processed\\\\images'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "from trimesh.transformations import euler_matrix # Added for rotate_rays\n",
    "\n",
    "# If you have the data package\n",
    "sys.path.append(\"..\")\n",
    "try:\n",
    "    from data.pollen_dataset import PollenDataset, get_train_test_split\n",
    "except ImportError:\n",
    "    # Fallback if not available\n",
    "    PollenDataset = None\n",
    "    get_train_test_split = None\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Positional Encoding (Reduced Frequencies)\n",
    "# -----------------------------------------------------------------------------\n",
    "def positional_encoding(x, L=4):\n",
    "    \"\"\"\n",
    "    Encode coordinates x with sine/cosine functions at increasing frequencies.\n",
    "    We use L=4 here (fewer than the classic L=10) to reduce high-frequency overfitting.\n",
    "    \"\"\"\n",
    "    out = [x]\n",
    "    for i in range(L):\n",
    "        for fn in (torch.sin, torch.cos):\n",
    "            out.append(fn((2.0**i) * np.pi * x))\n",
    "    return torch.cat(out, dim=-1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. NeRF Model\n",
    "# -----------------------------------------------------------------------------\n",
    "class NeRF(nn.Module):\n",
    "    def __init__(self, D=6, W=128, L=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            D: Number of hidden layers\n",
    "            W: Number of hidden units per layer\n",
    "            L: Positional encoding frequency levels\n",
    "        \"\"\"\n",
    "        super(NeRF, self).__init__()\n",
    "        self.L = L\n",
    "        in_ch = 3 * (2 * L + 1)  # 3 coords * (2L + 1)\n",
    "        layers = [nn.Linear(in_ch, W)] + [nn.Linear(W, W) for _ in range(D - 1)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(W, 4)\n",
    "        # Initialize sigma bias to something non-zero\n",
    "        with torch.no_grad():\n",
    "            self.output_layer.bias[3] = 0.1\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: x is (N, 3), output is (N, 4) => [R, G, B, sigma].\n",
    "        \"\"\"\n",
    "        x_enc = positional_encoding(x, self.L)\n",
    "        h = x_enc\n",
    "        for l in self.layers:\n",
    "            h = torch.relu(l(h))\n",
    "        return self.output_layer(h)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Render Rays (RGB + Alpha)\n",
    "# -----------------------------------------------------------------------------\n",
    "def render_rays(\n",
    "    model, rays_o, rays_d, near=0.5, far=1.5, N_samples=128, sigma_scale=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Volumetric rendering for a batch of rays:\n",
    "      - Sample points along each ray\n",
    "      - Query MLP for color (rgb) and density (sigma)\n",
    "      - Composite color and alpha\n",
    "    \"\"\"\n",
    "    device = rays_o.device\n",
    "    z_vals = torch.linspace(near, far, N_samples, device=device)\n",
    "\n",
    "    pts = (\n",
    "        rays_o[:, None, :] + rays_d[:, None, :] * z_vals[None, :, None]\n",
    "    )  # (B, N_samples, 3)\n",
    "    pts_flat = pts.reshape(-1, 3)\n",
    "\n",
    "    raw = model(pts_flat).reshape(pts.shape[0], N_samples, 4)\n",
    "    rgb = torch.sigmoid(raw[..., :3])\n",
    "    sigma = torch.relu(raw[..., 3]) * sigma_scale\n",
    "\n",
    "    deltas = z_vals[1:] - z_vals[:-1]\n",
    "    deltas = torch.cat([deltas, torch.tensor([1e10], device=device)])\n",
    "    deltas = deltas[None, :].expand(sigma.shape)\n",
    "\n",
    "    alpha = 1.0 - torch.exp(-sigma * deltas)\n",
    "    T = torch.cumprod(\n",
    "        torch.cat(\n",
    "            [torch.ones((sigma.shape[0], 1), device=device), 1.0 - alpha + 1e-10],\n",
    "            dim=-1,\n",
    "        ),\n",
    "        dim=-1,\n",
    "    )[:, :-1]\n",
    "    weights = alpha * T\n",
    "\n",
    "    rgb_map = torch.sum(weights[..., None] * rgb, dim=1)\n",
    "    alpha_map = torch.sum(weights, dim=1)\n",
    "    return rgb_map, alpha_map\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Losses: Silhouette, Spherical Prior, etc.\n",
    "# -----------------------------------------------------------------------------\n",
    "def silhouette_loss(alpha, mask):\n",
    "    \"\"\"Calculates the L2 loss between predicted alpha and ground truth mask.\"\"\"\n",
    "    return torch.mean((alpha - mask) ** 2)\n",
    "\n",
    "\n",
    "def spherical_prior_loss(\n",
    "    model, num_samples=2000, bound=1.0, desired_radius=0.6, sigma_scale=2.0, device=None\n",
    "):\n",
    "    \"\"\"Encourages density to concentrate near a specific radius.\"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    coords = torch.rand(num_samples, 3, device=device) * (2 * bound) - bound\n",
    "\n",
    "    raw = model(coords)\n",
    "    sigma = torch.relu(raw[..., 3]) * sigma_scale\n",
    "    dists = torch.norm(coords, dim=1)\n",
    "    # Encourage high sigma near the desired radius\n",
    "    loss = torch.mean(sigma * (dists - desired_radius) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def foreground_density_loss(alpha_map, mask, target_density=1.0):\n",
    "    \"\"\"Encourages the integrated density within the mask to be high.\"\"\"\n",
    "    eps = 1e-6\n",
    "    # D is approximate integrated density along the ray\n",
    "    D = -torch.log(1.0 - alpha_map + eps)\n",
    "    fg_mask = mask > 0.5\n",
    "    if torch.sum(fg_mask) > 0:\n",
    "        # Penalize density lower than target_density within the mask\n",
    "        return torch.mean(torch.clamp(target_density - D[fg_mask], min=0.0))\n",
    "    else:\n",
    "        # No foreground pixels in this batch\n",
    "        return torch.tensor(0.0, device=alpha_map.device)\n",
    "\n",
    "\n",
    "def smoothness_prior_loss(\n",
    "    model, num_samples=2000, bound=1.0, offset=0.01, sigma_scale=2.0, device=None\n",
    "):\n",
    "    \"\"\"Encourages adjacent points in space to have similar density (sigma).\"\"\"\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    coords = torch.rand(num_samples, 3, device=device) * (2 * bound) - bound\n",
    "\n",
    "    raw_center = model(coords)\n",
    "    sigma_center = torch.relu(raw_center[..., 3]) * sigma_scale\n",
    "\n",
    "    # Sample neighbors along axes\n",
    "    offsets = torch.tensor(\n",
    "        [\n",
    "            [offset, 0, 0],\n",
    "            [-offset, 0, 0],\n",
    "            [0, offset, 0],\n",
    "            [0, -offset, 0],\n",
    "            [0, 0, offset],\n",
    "            [0, 0, -offset],\n",
    "        ],\n",
    "        device=device,\n",
    "    ).float()\n",
    "\n",
    "    total_diff = 0.0\n",
    "    for off in offsets:\n",
    "        neighbor_coords = coords + off\n",
    "        # Clamp coordinates to stay within bounds (optional but can help)\n",
    "        # neighbor_coords = torch.clamp(neighbor_coords, -bound, bound)\n",
    "        raw_neighbor = model(neighbor_coords)\n",
    "        sigma_neighbor = torch.relu(raw_neighbor[..., 3]) * sigma_scale\n",
    "        # L2 difference in sigma between center and neighbor\n",
    "        total_diff += torch.mean((sigma_center - sigma_neighbor) ** 2)\n",
    "\n",
    "    # Average difference across all offset directions\n",
    "    return total_diff / offsets.shape[0]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. Ray Generation (Two Orthogonal Views)\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_rays(H, W, focal=300.0):\n",
    "    \"\"\"\n",
    "    Returns canonical (unrotated) front‐and‐side rays:\n",
    "      rays_o_front, rays_d_front, rays_o_side, rays_d_side\n",
    "      all as (H*W, 3) tensors.\n",
    "    \"\"\"\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.linspace(0, W - 1, W),\n",
    "        torch.linspace(0, H - 1, H),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    # front view (looking along -Z)\n",
    "    dirs_f = torch.stack(\n",
    "        [(i - W / 2.0) / focal, -(j - H / 2.0) / focal, -torch.ones_like(i)],\n",
    "        dim=-1,\n",
    "    )\n",
    "    rays_d_f = dirs_f / torch.norm(dirs_f, dim=-1, keepdim=True)\n",
    "    rays_o_f = torch.zeros_like(rays_d_f) # Origin at (0,0,0)\n",
    "\n",
    "    # side view (looking along -X from X=-1.5)\n",
    "    dirs_s = torch.stack(\n",
    "        [torch.ones_like(i), -(j - H / 2.0) / focal, -(i - W / 2.0) / focal],\n",
    "        dim=-1,\n",
    "    )\n",
    "    rays_d_s = dirs_s / torch.norm(dirs_s, dim=-1, keepdim=True)\n",
    "    rays_o_s = torch.zeros_like(rays_d_s)\n",
    "    rays_o_s[..., 0] = -1.5 # Origin offset along X\n",
    "\n",
    "    return (\n",
    "        rays_o_f.reshape(-1, 3),\n",
    "        rays_d_f.reshape(-1, 3),\n",
    "        rays_o_s.reshape(-1, 3),\n",
    "        rays_d_s.reshape(-1, 3),\n",
    "    )\n",
    "\n",
    "\n",
    "def rotate_rays(rays_o, rays_d, euler_angles):\n",
    "    \"\"\"\n",
    "    Apply the sample's rotation (in radians) to both origins and directions.\n",
    "    euler_angles: tensor([rx, ry, rz]) in radians, in 'sxyz' convention.\n",
    "    \"\"\"\n",
    "    # build 4×4 rotation matrix using trimesh, then extract the 3×3 upper‐left block\n",
    "    R4 = euler_matrix(\n",
    "        float(euler_angles[0]), # rx\n",
    "        float(euler_angles[1]), # ry\n",
    "        float(euler_angles[2]), # rz\n",
    "        \"sxyz\", # Euler convention\n",
    "    )\n",
    "    R = torch.from_numpy(R4[:3, :3]).to(rays_o.device).float()\n",
    "\n",
    "    # rotate origins & directions by multiplying with the rotation matrix\n",
    "    # (R @ vector.T).T is efficient for batch matrix-vector multiplication\n",
    "    ro = (R @ rays_o.T).T\n",
    "    rd = (R @ rays_d.T).T\n",
    "    return ro, rd\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Weighted Ray Sampling (Edges + Foreground)\n",
    "# -----------------------------------------------------------------------------\n",
    "def sample_rays_weighted(rays_o, rays_d, rgb, mask, original_shape, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Sample rays with higher probability at silhouette edges and foreground.\n",
    "    Handles cases with one or two views concatenated in the input tensors.\n",
    "    \"\"\"\n",
    "    H, W = original_shape\n",
    "    total_pixels = mask.shape[0]\n",
    "    pixels_per_view = H * W\n",
    "\n",
    "    if total_pixels == 2 * pixels_per_view: # Check if two views are present\n",
    "        # Two views concatenated (e.g., front and side)\n",
    "        weights_list = []\n",
    "        for view_idx in range(2):\n",
    "            start_idx = view_idx * pixels_per_view\n",
    "            end_idx = start_idx + pixels_per_view\n",
    "            view_mask = mask[start_idx:end_idx]\n",
    "            mask_2d = view_mask.reshape(H, W)\n",
    "\n",
    "            # Use a simple edge detection kernel (Laplacian)\n",
    "            kernel = (\n",
    "                torch.tensor(\n",
    "                    [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], device=mask.device\n",
    "                ).float()\n",
    "                / 8.0 # Normalize slightly\n",
    "            )\n",
    "            kernel = kernel.reshape(1, 1, 3, 3) # Add batch and channel dims\n",
    "\n",
    "            # Apply convolution to find edges\n",
    "            edges = torch.abs(\n",
    "                torch.nn.functional.conv2d(\n",
    "                    mask_2d.reshape(1, 1, H, W), kernel, padding=1 # Pad to keep size\n",
    "                )\n",
    "            ).reshape(H, W)\n",
    "\n",
    "            # Combine edge weights and foreground weights\n",
    "            edge_weights = edges.reshape(-1) + 0.1 # Add small base weight\n",
    "            fg_weights = (view_mask > 0.5).float() * 2.0 # Higher weight for foreground\n",
    "            weights = edge_weights + fg_weights\n",
    "            weights_list.append(weights)\n",
    "        # Concatenate weights from both views\n",
    "        weights = torch.cat(weights_list)\n",
    "    else:\n",
    "        # Single view fallback (or if total_pixels doesn't match 2*H*W)\n",
    "        mask_2d = mask.reshape(H, W)\n",
    "        kernel = (\n",
    "            torch.tensor(\n",
    "                [[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], device=mask.device\n",
    "            ).float()\n",
    "            / 8.0\n",
    "        )\n",
    "        kernel = kernel.reshape(1, 1, 3, 3)\n",
    "        edges = torch.abs(\n",
    "            torch.nn.functional.conv2d(mask_2d.reshape(1, 1, H, W), kernel, padding=1)\n",
    "        ).reshape(H, W)\n",
    "        edge_weights = edges.reshape(-1) + 0.1\n",
    "        fg_weights = (mask > 0.5).float() * 2.0\n",
    "        weights = edge_weights + fg_weights\n",
    "\n",
    "    # Normalize weights to get probabilities\n",
    "    p = weights / weights.sum()\n",
    "    # Sample indices based on probabilities\n",
    "    idx = torch.multinomial(p, batch_size, replacement=True)\n",
    "\n",
    "    # Return the sampled rays and corresponding pixel data\n",
    "    return rays_o[idx], rays_d[idx], rgb[idx], mask[idx]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. Debug Rendering (with Extra Mask Comparison)\n",
    "# -----------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def debug_render(\n",
    "    model,\n",
    "    rays_o,\n",
    "    rays_d,\n",
    "    H,\n",
    "    W,\n",
    "    near=0.5,\n",
    "    far=1.5,\n",
    "    sigma_scale=2.0,\n",
    "    N_samples=64, # Fewer samples for faster debug rendering\n",
    "    device=None,\n",
    "    title_prefix=\"debug\",\n",
    "    iteration=0,\n",
    "    out_dir=\"debug_renders\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Render the entire image (front or side) for debugging.\n",
    "    Saves color and alpha maps.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = rays_o.device\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    B = rays_o.shape[0]\n",
    "    chunk_size = 2048 # Process in chunks to avoid OOM\n",
    "    all_rgb = []\n",
    "    all_alpha = []\n",
    "\n",
    "    for start in range(0, B, chunk_size):\n",
    "        end = start + chunk_size\n",
    "        rgb_chunk, alpha_chunk = render_rays(\n",
    "            model,\n",
    "            rays_o[start:end],\n",
    "            rays_d[start:end],\n",
    "            near=near,\n",
    "            far=far,\n",
    "            sigma_scale=sigma_scale,\n",
    "            N_samples=N_samples,\n",
    "        )\n",
    "        all_rgb.append(rgb_chunk)\n",
    "        all_alpha.append(alpha_chunk)\n",
    "\n",
    "    # Concatenate chunks and reshape to image dimensions\n",
    "    rgb_full = torch.cat(all_rgb, dim=0).reshape(H, W, 3).cpu().numpy()\n",
    "    alpha_full = torch.cat(all_alpha, dim=0).reshape(H, W).cpu().numpy()\n",
    "    model.train() # Set model back to train mode\n",
    "\n",
    "    # 1. Save the RGB image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(np.clip(rgb_full, 0, 1)) # Clip values to [0, 1]\n",
    "    plt.title(f\"{title_prefix}_rgb_iter_{iteration}\")\n",
    "    plt.axis(\"off\")\n",
    "    rgb_path = os.path.join(out_dir, f\"{title_prefix}_rgb_iter_{iteration}.png\")\n",
    "    plt.savefig(rgb_path)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Save the alpha map\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(alpha_full, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(f\"{title_prefix}_alpha_iter_{iteration}\")\n",
    "    plt.axis(\"off\")\n",
    "    alpha_path = os.path.join(out_dir, f\"{title_prefix}_alpha_iter_{iteration}.png\")\n",
    "    plt.savefig(alpha_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(\n",
    "        f\"[DEBUG RENDER] Saved {title_prefix} images at iter {iteration} in {out_dir}/\"\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def debug_compare_mask_and_alpha(\n",
    "    model,\n",
    "    rays_o,\n",
    "    rays_d,\n",
    "    mask, # Ground truth mask for comparison\n",
    "    H,\n",
    "    W,\n",
    "    near=0.5,\n",
    "    far=1.5,\n",
    "    sigma_scale=2.0,\n",
    "    N_samples=64,\n",
    "    device=None,\n",
    "    title_prefix=\"debug\",\n",
    "    iteration=0,\n",
    "    out_dir=\"debug_renders\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Render alpha for all rays, then show a side-by-side comparison\n",
    "    of predicted alpha vs. ground-truth mask for debugging.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = rays_o.device\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    # Render alpha\n",
    "    B = rays_o.shape[0]\n",
    "    chunk_size = 2048\n",
    "    all_alpha = []\n",
    "    for start in range(0, B, chunk_size):\n",
    "        end = start + chunk_size\n",
    "        _, alpha_chunk = render_rays( # Only need alpha\n",
    "            model,\n",
    "            rays_o[start:end],\n",
    "            rays_d[start:end],\n",
    "            near=near,\n",
    "            far=far,\n",
    "            sigma_scale=sigma_scale,\n",
    "            N_samples=N_samples,\n",
    "        )\n",
    "        all_alpha.append(alpha_chunk)\n",
    "    alpha_full = torch.cat(all_alpha, dim=0).reshape(H, W).cpu().numpy()\n",
    "    model.train() # Set model back to train mode\n",
    "\n",
    "    # Reshape the ground-truth mask as well\n",
    "    mask_gt = mask.reshape(H, W).cpu().numpy()\n",
    "\n",
    "    # Plot side-by-side\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(mask_gt, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(f\"{title_prefix} GT Mask (iter={iteration})\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(alpha_full, cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.title(f\"{title_prefix} Predicted Alpha (iter={iteration})\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    compare_path = os.path.join(\n",
    "        out_dir, f\"{title_prefix}_mask_vs_alpha_iter_{iteration}.png\"\n",
    "    )\n",
    "    plt.savefig(compare_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(\n",
    "        f\"[DEBUG] Saved mask-vs-alpha comparison for {title_prefix} at iter {iteration} in {out_dir}/\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. 3D Extraction via Marching Cubes\n",
    "# -----------------------------------------------------------------------------\n",
    "def extract_3d_from_nerf(\n",
    "    model, resolution=128, bound=1.0, sigma_scale=2.0, device=None, sigma_threshold=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts a 3D mesh from the learned NeRF density field using marching cubes.\n",
    "    \"\"\"\n",
    "    print(\"\\n[EXTRACT 3D] Running marching cubes...\")\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # Create a grid of coordinates covering the volume\n",
    "    coords = (\n",
    "        torch.stack(\n",
    "            torch.meshgrid(\n",
    "                torch.linspace(-bound, bound, resolution, device=device),\n",
    "                torch.linspace(-bound, bound, resolution, device=device),\n",
    "                torch.linspace(-bound, bound, resolution, device=device),\n",
    "                indexing=\"ij\", # Important: use 'ij' indexing for marching cubes\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "        .reshape(-1, 3)\n",
    "    )\n",
    "\n",
    "    # Query the NeRF model for density (sigma) at each grid point\n",
    "    sigmas = []\n",
    "    chunk = 4096 # Process in chunks\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, coords.shape[0], chunk):\n",
    "            end = start + chunk\n",
    "            out = model(coords[start:end])\n",
    "            # Apply ReLU and scale sigma as done during rendering\n",
    "            sigma_part = torch.relu(out[..., 3]) * sigma_scale\n",
    "            sigmas.append(sigma_part.cpu()) # Move to CPU to save GPU memory\n",
    "    # Reshape the flat sigma values into a 3D volume\n",
    "    sigma_volume = torch.cat(sigmas).reshape(resolution, resolution, resolution).numpy()\n",
    "\n",
    "    # Determine the iso-level for marching cubes\n",
    "    vol_min, vol_max = sigma_volume.min(), sigma_volume.max()\n",
    "    vol_mean, vol_std = sigma_volume.mean(), sigma_volume.std()\n",
    "    print(\n",
    "        f\"  Sigma volume stats: min={vol_min:.4f}, max={vol_max:.4f}, mean={vol_mean:.4f}, std={vol_std:.4f}\"\n",
    "    )\n",
    "\n",
    "    if sigma_threshold is None:\n",
    "        # Heuristic: level slightly above the mean density\n",
    "        level = vol_mean + 0.3 * vol_std\n",
    "        # Ensure level is within the range of observed sigma values\n",
    "        if (level <= vol_min) or (level >= vol_max):\n",
    "            level = vol_mean # Fallback to mean if heuristic is out of bounds\n",
    "        print(f\"  Using iso-level={level:.4f} (heuristic)\")\n",
    "    else:\n",
    "        level = sigma_threshold\n",
    "        print(f\"  Using iso-level={level:.4f} (user-defined)\")\n",
    "\n",
    "\n",
    "    mesh = None\n",
    "    try:\n",
    "        # Run marching cubes\n",
    "        verts, faces, normals, _ = marching_cubes(sigma_volume, level=level)\n",
    "\n",
    "        # Rescale vertices from grid coordinates [0, resolution-1] to world coordinates [-bound, bound]\n",
    "        verts = (verts / (resolution - 1.0)) * (2.0 * bound) - bound # More accurate scaling\n",
    "\n",
    "        mesh = trimesh.Trimesh(vertices=verts, faces=faces, process=False) # process=False avoids auto-fixing\n",
    "        # Attempt to fix inverted faces if normals are not provided or inconsistent\n",
    "        mesh.fix_normals()\n",
    "        mesh.fill_holes() # Optional: try to fill holes\n",
    "        mesh.remove_unreferenced_vertices() # Clean up\n",
    "\n",
    "        # Check if mesh is valid after processing\n",
    "        if not mesh.is_watertight:\n",
    "            print(\"  Warning: Extracted mesh is not watertight.\")\n",
    "        if len(mesh.vertices) == 0 or len(mesh.faces) == 0:\n",
    "             print(\"  Warning: Marching cubes resulted in an empty mesh.\")\n",
    "             mesh = None # Treat as failure\n",
    "        else:\n",
    "             mesh.export(\"nerf_reconstruction.stl\")\n",
    "             print(f\"  --> Saved mesh ({len(mesh.vertices)} verts, {len(mesh.faces)} faces) to nerf_reconstruction.stl\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Marching cubes error: {e}\")\n",
    "        print(f\"  Failed to extract mesh at level {level:.4f}.\")\n",
    "        mesh = None # Ensure mesh is None on failure\n",
    "\n",
    "    model.train() # Set model back to train mode\n",
    "    return mesh\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. Chamfer Distance\n",
    "# -----------------------------------------------------------------------------\n",
    "def chamfer_distance(points1, points2, num_points_sample=None):\n",
    "    \"\"\"\n",
    "    Calculates the Chamfer distance between two point clouds.\n",
    "    Optionally subsamples points for faster computation.\n",
    "    points1, points2: numpy arrays of shape (N, 3) and (M, 3)\n",
    "    \"\"\"\n",
    "    if num_points_sample:\n",
    "        if points1.shape[0] > num_points_sample:\n",
    "            idx1 = np.random.choice(points1.shape[0], num_points_sample, replace=False)\n",
    "            points1 = points1[idx1]\n",
    "        if points2.shape[0] > num_points_sample:\n",
    "            idx2 = np.random.choice(points2.shape[0], num_points_sample, replace=False)\n",
    "            points2 = points2[idx2]\n",
    "\n",
    "    if points1.shape[0] == 0 or points2.shape[0] == 0:\n",
    "        print(\"[Chamfer] Warning: One or both point clouds are empty.\")\n",
    "        return float('inf') # Or handle as appropriate\n",
    "\n",
    "    # Build KD-Trees for efficient nearest neighbor search\n",
    "    tree1 = cKDTree(points1)\n",
    "    tree2 = cKDTree(points2)\n",
    "\n",
    "    # Find nearest neighbor distances:\n",
    "    # d1: distances from points2 to their nearest neighbor in points1\n",
    "    # d2: distances from points1 to their nearest neighbor in points2\n",
    "    d1, _ = tree1.query(points2)\n",
    "    d2, _ = tree2.query(points1)\n",
    "\n",
    "    # Chamfer distance is the sum of the mean squared distances\n",
    "    chamfer_dist = np.mean(d1**2) + np.mean(d2**2)\n",
    "    return chamfer_dist\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10. Plot Meshes\n",
    "# -----------------------------------------------------------------------------\n",
    "def plot_meshes(\n",
    "    gt_mesh, pred_mesh, outpath=\"mesh_comparison.png\", title=\"Mesh Comparison\"\n",
    "):\n",
    "    \"\"\"Plots ground truth and predicted meshes side-by-side after centering and scaling.\"\"\"\n",
    "    from mpl_toolkits.mplot3d import Axes3D # Local import for plotting\n",
    "\n",
    "    if gt_mesh is None or pred_mesh is None:\n",
    "        print(\"[PLOT] Cannot plot, one or both meshes are None.\")\n",
    "        return\n",
    "    if len(gt_mesh.vertices) == 0 or len(pred_mesh.vertices) == 0:\n",
    "        print(\"[PLOT] Cannot plot, one or both meshes have no vertices.\")\n",
    "        return\n",
    "    if len(gt_mesh.faces) == 0 or len(pred_mesh.faces) == 0:\n",
    "        print(\"[PLOT] Cannot plot, one or both meshes have no faces (needed for plot_trisurf).\")\n",
    "        # Optionally, could plot point clouds instead using ax.scatter\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6)) # Wider figure for side-by-side\n",
    "\n",
    "    # --- Plot Ground Truth Mesh ---\n",
    "    ax1 = fig.add_subplot(121, projection=\"3d\")\n",
    "    # Center the mesh\n",
    "    center_gt = gt_mesh.vertices.mean(axis=0)\n",
    "    vertices_gt_centered = gt_mesh.vertices - center_gt\n",
    "    # Scale to fit in unit cube (approx)\n",
    "    scale_gt = np.max(np.linalg.norm(vertices_gt_centered, axis=1))\n",
    "    vertices_gt_norm = vertices_gt_centered / scale_gt if scale_gt > 1e-6 else vertices_gt_centered\n",
    "\n",
    "    ax1.plot_trisurf(\n",
    "        vertices_gt_norm[:, 0],\n",
    "        vertices_gt_norm[:, 1],\n",
    "        vertices_gt_norm[:, 2],\n",
    "        triangles=gt_mesh.faces,\n",
    "        color=\"blue\",\n",
    "        alpha=0.7,\n",
    "        edgecolor='gray', # Add edges for clarity\n",
    "        linewidth=0.1\n",
    "    )\n",
    "    ax1.set_title(\"Ground Truth Mesh\")\n",
    "    ax1.set_box_aspect([1, 1, 1]) # Equal aspect ratio\n",
    "    ax1.set_xlabel(\"X\"); ax1.set_ylabel(\"Y\"); ax1.set_zlabel(\"Z\")\n",
    "    ax1.set_xlim([-1, 1]); ax1.set_ylim([-1, 1]); ax1.set_zlim([-1, 1]) # Consistent bounds\n",
    "\n",
    "    # --- Plot Predicted Mesh ---\n",
    "    ax2 = fig.add_subplot(122, projection=\"3d\")\n",
    "    # Center the mesh\n",
    "    center_pred = pred_mesh.vertices.mean(axis=0)\n",
    "    vertices_pred_centered = pred_mesh.vertices - center_pred\n",
    "    # Scale to fit in unit cube (approx)\n",
    "    scale_pred = np.max(np.linalg.norm(vertices_pred_centered, axis=1))\n",
    "    vertices_pred_norm = vertices_pred_centered / scale_pred if scale_pred > 1e-6 else vertices_pred_centered\n",
    "\n",
    "    ax2.plot_trisurf(\n",
    "        vertices_pred_norm[:, 0],\n",
    "        vertices_pred_norm[:, 1],\n",
    "        vertices_pred_norm[:, 2],\n",
    "        triangles=pred_mesh.faces,\n",
    "        color=\"red\",\n",
    "        alpha=0.7,\n",
    "        edgecolor='gray',\n",
    "        linewidth=0.1\n",
    "    )\n",
    "    ax2.set_title(\"Predicted Mesh (NeRF)\")\n",
    "    ax2.set_box_aspect([1, 1, 1]) # Equal aspect ratio\n",
    "    ax2.set_xlabel(\"X\"); ax2.set_ylabel(\"Y\"); ax2.set_zlabel(\"Z\")\n",
    "    ax2.set_xlim([-1, 1]); ax2.set_ylim([-1, 1]); ax2.set_zlim([-1, 1]) # Consistent bounds\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to fit suptitle\n",
    "    plt.savefig(outpath)\n",
    "    # plt.show() # Uncomment to display plot interactively\n",
    "    plt.close()\n",
    "    print(f\"[PLOT] Saved mesh comparison to {outpath}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 11. Training Loop (with Additional Debug)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_nerf(\n",
    "    model,\n",
    "    rays_o_all,\n",
    "    rays_d_all,\n",
    "    target_pixels_all,\n",
    "    mask_all,\n",
    "    image_shape, # Tuple (H, W)\n",
    "    num_iterations=8000,\n",
    "    device=None,\n",
    "    near=0.5,\n",
    "    far=1.5,\n",
    "    sigma_scale=2.0,\n",
    "    debug_interval=1000,\n",
    "    out_dir=\"debug_renders\",\n",
    "    lr=5e-4,\n",
    "    batch_size=1024,\n",
    "    N_samples_train=64, # Samples per ray during training\n",
    "    N_samples_debug=128, # Samples per ray for debug renders\n",
    "    lambda_photo=1.0, # Weight for RGB photo loss\n",
    "    lambda_sil=1.0,   # Weight for silhouette loss\n",
    "    lambda_shape=1e-3, # Weight for spherical prior loss\n",
    "    lambda_density=0.2, # Weight for foreground density loss\n",
    "    lambda_smooth=0.1, # Weight for smoothness loss\n",
    "    desired_radius=0.6 # Target radius for spherical prior\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the NeRF model using provided ray data and masks.\n",
    "\n",
    "    Features:\n",
    "    - Weighted ray sampling (emphasizing edges and foreground).\n",
    "    - Combined loss: photo, silhouette, spherical prior, density, smoothness.\n",
    "    - Periodic debug rendering (RGB, alpha, mask comparison).\n",
    "    - Learning rate scheduling and AMP (Automatic Mixed Precision).\n",
    "    - Saves the best model based on validation loss (using total loss here).\n",
    "    \"\"\"\n",
    "    H, W = image_shape\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Scheduler reduces LR if loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=500, verbose=True # Increased patience\n",
    "    )\n",
    "    # AMP for faster training and less memory usage\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "    print(\"[TRAIN] Loss weights:\")\n",
    "    print(f\"  Photo:    {lambda_photo}\")\n",
    "    print(f\"  Sil:      {lambda_sil}\")\n",
    "    print(f\"  Shape:    {lambda_shape}\")\n",
    "    print(f\"  Density:  {lambda_density}\")\n",
    "    print(f\"  Smooth:   {lambda_smooth}\")\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    print(\"\\n[TRAIN] Starting training...\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    t_range = trange(num_iterations, desc=\"Training\", unit=\"iter\")\n",
    "    for i in t_range:\n",
    "        model.train() # Ensure model is in training mode\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sample a batch of rays using weighted strategy\n",
    "        rays_o_batch, rays_d_batch, rgb_batch, mask_batch = sample_rays_weighted(\n",
    "            rays_o_all,\n",
    "            rays_d_all,\n",
    "            target_pixels_all,\n",
    "            mask_all,\n",
    "            original_shape=(H, W),\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Use AMP context manager\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            # Render rays for the sampled batch\n",
    "            rgb_map, alpha_map = render_rays(\n",
    "                model,\n",
    "                rays_o_batch,\n",
    "                rays_d_batch,\n",
    "                near=near,\n",
    "                far=far,\n",
    "                sigma_scale=sigma_scale,\n",
    "                N_samples=N_samples_train, # Use training sample count\n",
    "            )\n",
    "\n",
    "            # Calculate individual loss components\n",
    "            # 1. Photometric Loss (RGB difference) - only where mask is > 0? Optional.\n",
    "            # photo_loss = torch.mean((rgb_map[mask_batch > 0.5] - rgb_batch[mask_batch > 0.5]) ** 2) if torch.sum(mask_batch > 0.5) > 0 else torch.tensor(0.0, device=device)\n",
    "            photo_loss = torch.mean((rgb_map - rgb_batch)**2) # Simpler: use all pixels\n",
    "\n",
    "            # 2. Silhouette Loss (Alpha vs Mask)\n",
    "            sil_loss_val = silhouette_loss(alpha_map, mask_batch)\n",
    "\n",
    "            # 3. Spherical Shape Prior Loss\n",
    "            shape_loss_val = spherical_prior_loss(\n",
    "                model,\n",
    "                num_samples=1024, # Fewer samples okay within training loop\n",
    "                bound=1.0, # Volume bounds\n",
    "                desired_radius=desired_radius,\n",
    "                sigma_scale=sigma_scale,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            # 4. Foreground Density Loss\n",
    "            dens_loss_val = foreground_density_loss(\n",
    "                alpha_map, mask_batch, target_density=1.0 # Encourage high density where mask is 1\n",
    "            )\n",
    "\n",
    "            # 5. Smoothness Prior Loss (Density smoothness)\n",
    "            smooth_loss_val = smoothness_prior_loss(\n",
    "                model,\n",
    "                num_samples=1024, # Fewer samples\n",
    "                bound=1.0,\n",
    "                offset=0.02, # Slightly larger offset might be okay\n",
    "                sigma_scale=sigma_scale,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # Combine losses with their respective weights\n",
    "            total_loss = (\n",
    "                lambda_photo * photo_loss\n",
    "                + lambda_sil * sil_loss_val\n",
    "                + lambda_shape * shape_loss_val\n",
    "                + lambda_density * dens_loss_val\n",
    "                + lambda_smooth * smooth_loss_val\n",
    "            )\n",
    "\n",
    "        # Backpropagation using AMP scaler\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        # Update learning rate based on total loss\n",
    "        scheduler.step(total_loss)\n",
    "\n",
    "        # --- Logging ---\n",
    "        if (i + 1) % 200 == 0:\n",
    "            t_range.set_postfix(\n",
    "                loss=f\"{total_loss.item():.4f}\",\n",
    "                photo=f\"{photo_loss.item():.4f}\",\n",
    "                sil=f\"{sil_loss_val.item():.4f}\",\n",
    "                shape=f\"{shape_loss_val.item():.4f}\",\n",
    "                dens=f\"{dens_loss_val.item():.4f}\",\n",
    "                smooth=f\"{smooth_loss_val.item():.4f}\",\n",
    "                lr=f\"{optimizer.param_groups[0]['lr']:.1e}\"\n",
    "            )\n",
    "            # Save best model based on total loss\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                torch.save(model.state_dict(), \"nerf_best_model.pth\")\n",
    "                # print(f\"[TRAIN @ Iter {i+1}] ==> New best model saved (Loss: {best_loss:.4f}).\")\n",
    "\n",
    "\n",
    "        # --- Debug Rendering & Checkpointing ---\n",
    "        if debug_interval > 0 and (i + 1) % debug_interval == 0:\n",
    "            print(f\"\\n[DEBUG @ Iter {i + 1}] Rendering debug images...\")\n",
    "            model.eval() # Switch to eval mode for rendering\n",
    "\n",
    "            # 1) Render front view (assuming first H*W rays are front)\n",
    "            if rays_o_all.shape[0] >= H * W:\n",
    "                front_rays_o = rays_o_all[: H * W]\n",
    "                front_rays_d = rays_d_all[: H * W]\n",
    "                front_mask = mask_all[: H * W]\n",
    "\n",
    "                debug_render(\n",
    "                    model, front_rays_o, front_rays_d, H, W,\n",
    "                    near=near, far=far, sigma_scale=sigma_scale, N_samples=N_samples_debug,\n",
    "                    device=device, title_prefix=\"front\", iteration=i + 1, out_dir=out_dir,\n",
    "                )\n",
    "                debug_compare_mask_and_alpha(\n",
    "                    model, front_rays_o, front_rays_d, front_mask, H, W,\n",
    "                    near=near, far=far, sigma_scale=sigma_scale, N_samples=N_samples_debug,\n",
    "                    device=device, title_prefix=\"front\", iteration=i + 1, out_dir=out_dir,\n",
    "                )\n",
    "\n",
    "            # 2) Render side view (assuming next H*W rays are side)\n",
    "            if rays_o_all.shape[0] >= 2 * H * W:\n",
    "                side_rays_o = rays_o_all[H * W : 2 * H * W]\n",
    "                side_rays_d = rays_d_all[H * W : 2 * H * W]\n",
    "                side_mask = mask_all[H * W : 2 * H * W]\n",
    "\n",
    "                debug_render(\n",
    "                    model, side_rays_o, side_rays_d, H, W,\n",
    "                    near=near, far=far, sigma_scale=sigma_scale, N_samples=N_samples_debug,\n",
    "                    device=device, title_prefix=\"side\", iteration=i + 1, out_dir=out_dir,\n",
    "                )\n",
    "                debug_compare_mask_and_alpha(\n",
    "                    model, side_rays_o, side_rays_d, side_mask, H, W,\n",
    "                    near=near, far=far, sigma_scale=sigma_scale, N_samples=N_samples_debug,\n",
    "                    device=device, title_prefix=\"side\", iteration=i + 1, out_dir=out_dir,\n",
    "                )\n",
    "\n",
    "            # Save checkpoint\n",
    "            ckpt_path = os.path.join(out_dir, f\"nerf_checkpoint_{i + 1}.pth\")\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\"[TRAIN] Saved checkpoint {ckpt_path}\\n\")\n",
    "            model.train() # Switch back to train mode\n",
    "\n",
    "    print(\"\\n[TRAIN] Training finished.\")\n",
    "    # Load the best performing model before returning\n",
    "    if os.path.exists(\"nerf_best_model.pth\"):\n",
    "        print(\"[TRAIN] Loading best model state dict.\")\n",
    "        model.load_state_dict(torch.load(\"nerf_best_model.pth\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 12. Main Execution Block\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    DATA_AVAILABLE = PollenDataset is not None and get_train_test_split is not None\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    IMAGE_SIZE = 128 # Assumed image size if dataset not loaded\n",
    "    FOCAL_LENGTH = 300.0 # Example focal length\n",
    "    NEAR_BOUND = 0.4   # Near plane for rendering\n",
    "    FAR_BOUND = 1.6    # Far plane for rendering\n",
    "    SIGMA_SCALE_TRAIN = 1.0 # Multiplier for sigma during training\n",
    "    SIGMA_SCALE_EXTRACT = 2.0 # Multiplier for sigma during mesh extraction (can differ)\n",
    "    MESH_RESOLUTION = 192 # Resolution for marching cubes grid\n",
    "    MESH_BOUND = 1.0 # Spatial bounds for mesh extraction [-bound, bound]\n",
    "    SIGMA_THRESHOLD_EXTRACT = None # Or set a specific float value, e.g. 5.0\n",
    "    NUM_ITERATIONS = 10000 # Training iterations\n",
    "    DEBUG_INTERVAL = 2000 # How often to save debug images/checkpoints (0 to disable)\n",
    "    DEBUG_DIR = \"debug_nerf_pollen\"\n",
    "\n",
    "    print(f\"[SYS] Using device: {DEVICE}\")\n",
    "\n",
    "    # --- 1. Load Data (if available) ---\n",
    "    if DATA_AVAILABLE:\n",
    "        print(\"[DATA] Loading PollenDataset...\")\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # Add resize if needed: transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        ])\n",
    "        # Assuming get_train_test_split returns dataset, train_ids, test_ids\n",
    "        # And dataset provides: (left_img, right_img), gt_mesh_data, rotations, voxels\n",
    "        # where gt_mesh_data could be points, mesh path, etc.\n",
    "        dataset, train_ids, test_ids = get_train_test_split(\n",
    "            image_transforms=image_transform,\n",
    "            mesh_transforms=None, # Assuming mesh is loaded directly if needed\n",
    "            device=DEVICE,\n",
    "            # Add base_dir if your dataset requires it\n",
    "            # base_dir=\"path/to/your/pollen/data\"\n",
    "        )\n",
    "\n",
    "        # Select a sample (e.g., the first training sample)\n",
    "        sample_idx = train_ids[0]\n",
    "        # Unpack the data - ADJUST THIS BASED ON YOUR PollenDataset.__getitem__\n",
    "        (left_img, right_img), gt_mesh_data, rotations, voxels = dataset[sample_idx]\n",
    "\n",
    "        # --- Ground Truth Mesh Handling ---\n",
    "        # Try to load the ground truth mesh if gt_mesh_data is a path or similar\n",
    "        gt_mesh = None\n",
    "        gt_points = None\n",
    "        if isinstance(gt_mesh_data, str) and os.path.exists(gt_mesh_data):\n",
    "            try:\n",
    "                gt_mesh = trimesh.load(gt_mesh_data, process=False)\n",
    "                gt_points = gt_mesh.sample(5000) # Sample points for Chamfer\n",
    "                print(f\"[DATA] Loaded GT mesh: {gt_mesh_data} ({len(gt_mesh.vertices)} verts, {len(gt_mesh.faces)} faces)\")\n",
    "            except Exception as e:\n",
    "                print(f\"[DATA] Warning: Could not load GT mesh from {gt_mesh_data}: {e}\")\n",
    "        elif isinstance(gt_mesh_data, torch.Tensor): # Assuming it's points if Tensor\n",
    "             gt_points = gt_mesh_data.cpu().numpy()\n",
    "             print(f\"[DATA] Loaded GT points: {gt_points.shape}\")\n",
    "        elif isinstance(gt_mesh_data, trimesh.Trimesh):\n",
    "             gt_mesh = gt_mesh_data\n",
    "             gt_points = gt_mesh.sample(5000)\n",
    "             print(f\"[DATA] Using provided GT trimesh object.\")\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"[DATA] Loaded sample #{sample_idx} -> \"\n",
    "            f\"Images: L={left_img.shape}, R={right_img.shape}; \"\n",
    "            f\"GT Points: {gt_points.shape if gt_points is not None else 'None'}; \"\n",
    "            f\"Rotations (rad): {rotations.tolist()}; \"\n",
    "            # f\"Voxels: {voxels.shape if hasattr(voxels, 'shape') else voxels}\" # Optional\n",
    "        )\n",
    "        H, W = left_img.shape[1], left_img.shape[2]\n",
    "        print(f\"[DATA] Image dimensions: H={H}, W={W}\")\n",
    "\n",
    "        # --- 2. Preprocess Images ---\n",
    "        # Ensure images are on the correct device and float type\n",
    "        left_img = left_img.to(DEVICE).float()\n",
    "        right_img = right_img.to(DEVICE).float()\n",
    "        rotations = rotations.to(DEVICE).float()\n",
    "\n",
    "        # Ensure 3 channels (repeat grayscale if needed)\n",
    "        if left_img.ndim == 3 and left_img.shape[0] == 1: left_img = left_img.repeat(3, 1, 1)\n",
    "        if right_img.ndim == 3 and right_img.shape[0] == 1: right_img = right_img.repeat(3, 1, 1)\n",
    "        if left_img.ndim == 2: left_img = left_img.unsqueeze(0).repeat(3, 1, 1) # Add channel and repeat\n",
    "        if right_img.ndim == 2: right_img = right_img.unsqueeze(0).repeat(3, 1, 1)\n",
    "\n",
    "        # Normalize to [0, 1] if they are in [0, 255]\n",
    "        if left_img.max() > 1.0: left_img /= 255.0\n",
    "        if right_img.max() > 1.0: right_img /= 255.0\n",
    "\n",
    "        # Display input images\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.subplot(1, 2, 1); plt.imshow(left_img.permute(1, 2, 0).cpu().clamp(0,1)); plt.title(\"Left Input Image\"); plt.axis(\"off\")\n",
    "        plt.subplot(1, 2, 2); plt.imshow(right_img.permute(1, 2, 0).cpu().clamp(0,1)); plt.title(\"Right Input Image\"); plt.axis(\"off\")\n",
    "        plt.tight_layout(); #plt.show();\n",
    "        plt.savefig(os.path.join(DEBUG_DIR, \"input_images.png\")); plt.close()\n",
    "        print(f\"[DATA] Saved input images view to {os.path.join(DEBUG_DIR, 'input_images.png')}\")\n",
    "\n",
    "\n",
    "        # --- 3. Create Masks (Silhouettes) ---\n",
    "        # Convert to grayscale, maybe blur slightly, then threshold\n",
    "        left_gray = left_img.mean(dim=0, keepdim=True)\n",
    "        right_gray = right_img.mean(dim=0, keepdim=True)\n",
    "        # Optional blurring (helps remove noise)\n",
    "        blur_kernel_size = 5\n",
    "        left_gray = torch.nn.functional.avg_pool2d(\n",
    "             left_gray.unsqueeze(0), blur_kernel_size, stride=1, padding=blur_kernel_size//2\n",
    "        ).squeeze()\n",
    "        right_gray = torch.nn.functional.avg_pool2d(\n",
    "             right_gray.unsqueeze(0), blur_kernel_size, stride=1, padding=blur_kernel_size//2\n",
    "        ).squeeze()\n",
    "        # Threshold to create binary mask (adjust threshold 0.2 if needed)\n",
    "        silhouette_threshold = 0.2\n",
    "        left_mask = (left_gray > silhouette_threshold).float().reshape(-1) # Flatten\n",
    "        right_mask = (right_gray > silhouette_threshold).float().reshape(-1) # Flatten\n",
    "\n",
    "        # Display masks\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.subplot(1, 2, 1); plt.imshow(left_mask.reshape(H, W).cpu(), cmap='gray'); plt.title(\"Left Mask\"); plt.axis(\"off\")\n",
    "        plt.subplot(1, 2, 2); plt.imshow(right_mask.reshape(H, W).cpu(), cmap='gray'); plt.title(\"Right Mask\"); plt.axis(\"off\")\n",
    "        plt.tight_layout(); #plt.show();\n",
    "        plt.savefig(os.path.join(DEBUG_DIR, \"input_masks.png\")); plt.close()\n",
    "        print(f\"[DATA] Saved input masks view to {os.path.join(DEBUG_DIR, 'input_masks.png')}\")\n",
    "\n",
    "        # --- 4. Prepare Ray Data ---\n",
    "        # Flatten target image pixels (N, 3)\n",
    "        left_img_flat = left_img.permute(1, 2, 0).reshape(-1, 3)\n",
    "        right_img_flat = right_img.permute(1, 2, 0).reshape(-1, 3)\n",
    "\n",
    "        # Generate canonical (unrotated) rays for front and side views\n",
    "        rays_o_f, rays_d_f, rays_o_s, rays_d_s = get_rays(H, W, focal=FOCAL_LENGTH)\n",
    "        rays_o_f, rays_d_f = rays_o_f.to(DEVICE), rays_d_f.to(DEVICE)\n",
    "        rays_o_s, rays_d_s = rays_o_s.to(DEVICE), rays_d_s.to(DEVICE)\n",
    "\n",
    "\n",
    "        # Rotate rays according to the sample's specific rotation\n",
    "        # Assuming 'left' view corresponds to 'front' canonical rays\n",
    "        # Assuming 'right' view corresponds to 'side' canonical rays\n",
    "        rays_o_left, rays_d_left = rotate_rays(rays_o_f, rays_d_f, rotations)\n",
    "        rays_o_right, rays_d_right = rotate_rays(rays_o_s, rays_d_s, rotations)\n",
    "\n",
    "        # Concatenate data from both views for training\n",
    "        rays_o_all = torch.cat([rays_o_left, rays_o_right], dim=0)\n",
    "        rays_d_all = torch.cat([rays_d_left, rays_d_right], dim=0)\n",
    "        target_pixels_all = torch.cat([left_img_flat, right_img_flat], dim=0)\n",
    "        mask_all = torch.cat([left_mask, right_mask], dim=0)\n",
    "\n",
    "        print(f\"[DATA] Prepared training data shapes:\")\n",
    "        print(f\"  rays_o_all: {rays_o_all.shape}\")\n",
    "        print(f\"  rays_d_all: {rays_d_all.shape}\")\n",
    "        print(f\"  target_pixels_all: {target_pixels_all.shape}\")\n",
    "        print(f\"  mask_all: {mask_all.shape}\")\n",
    "\n",
    "    else:\n",
    "        # --- Fallback: Create dummy data if dataset not loaded ---\n",
    "        print(\"[DATA] PollenDataset not found. Creating dummy data...\")\n",
    "        H, W = IMAGE_SIZE, IMAGE_SIZE\n",
    "        # Dummy images (e.g., white square on black bg)\n",
    "        left_img = torch.zeros(3, H, W, device=DEVICE)\n",
    "        right_img = torch.zeros(3, H, W, device=DEVICE)\n",
    "        left_img[:, H//4:3*H//4, W//4:3*W//4] = 1.0\n",
    "        right_img[:, H//4:3*H//4, W//4:3*W//4] = 1.0\n",
    "        # Dummy masks\n",
    "        left_mask = (left_img.mean(0) > 0.1).float().reshape(-1)\n",
    "        right_mask = (right_img.mean(0) > 0.1).float().reshape(-1)\n",
    "        # Dummy rotations (identity)\n",
    "        rotations = torch.zeros(3, device=DEVICE)\n",
    "        # Dummy GT points (random sphere)\n",
    "        phi = np.random.uniform(0, np.pi, 5000)\n",
    "        costheta = np.random.uniform(-1, 1, 5000)\n",
    "        theta = np.arccos(costheta)\n",
    "        x = 0.6 * np.sin(phi) * np.sin(theta)\n",
    "        y = 0.6 * np.sin(phi) * np.cos(theta)\n",
    "        z = 0.6 * np.cos(phi)\n",
    "        gt_points = np.stack([x, y, z], axis=-1)\n",
    "        gt_mesh = None # No dummy mesh\n",
    "\n",
    "\n",
    "        left_img_flat = left_img.permute(1, 2, 0).reshape(-1, 3)\n",
    "        right_img_flat = right_img.permute(1, 2, 0).reshape(-1, 3)\n",
    "        rays_o_f, rays_d_f, rays_o_s, rays_d_s = get_rays(H, W, focal=FOCAL_LENGTH)\n",
    "        rays_o_f, rays_d_f = rays_o_f.to(DEVICE), rays_d_f.to(DEVICE)\n",
    "        rays_o_s, rays_d_s = rays_o_s.to(DEVICE), rays_d_s.to(DEVICE)\n",
    "        rays_o_left, rays_d_left = rotate_rays(rays_o_f, rays_d_f, rotations)\n",
    "        rays_o_right, rays_d_right = rotate_rays(rays_o_s, rays_d_s, rotations)\n",
    "        rays_o_all = torch.cat([rays_o_left, rays_o_right], dim=0)\n",
    "        rays_d_all = torch.cat([rays_d_left, rays_d_right], dim=0)\n",
    "        target_pixels_all = torch.cat([left_img_flat, right_img_flat], dim=0)\n",
    "        mask_all = torch.cat([left_mask, right_mask], dim=0)\n",
    "\n",
    "\n",
    "    # --- 5. Initialize NeRF Model ---\n",
    "    model = NeRF(D=6, W=128, L=4).to(DEVICE) # Keep parameters relatively small\n",
    "    print(f\"[MODEL] Initialized NeRF (D=6, W=128, L=4) on {DEVICE}.\")\n",
    "\n",
    "    # --- 6. Train the Model ---\n",
    "    # Create debug directory if it doesn't exist\n",
    "    os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "    model = train_nerf(\n",
    "        model,\n",
    "        rays_o_all,\n",
    "        rays_d_all,\n",
    "        target_pixels_all,\n",
    "        mask_all,\n",
    "        image_shape=(H, W),\n",
    "        num_iterations=NUM_ITERATIONS,\n",
    "        device=DEVICE,\n",
    "        near=NEAR_BOUND,\n",
    "        far=FAR_BOUND,\n",
    "        sigma_scale=SIGMA_SCALE_TRAIN,\n",
    "        debug_interval=DEBUG_INTERVAL,\n",
    "        out_dir=DEBUG_DIR,\n",
    "        # Can adjust other train_nerf parameters here if needed\n",
    "        # lr=5e-4, batch_size=1024, lambda_sil=1.0, etc.\n",
    "    )\n",
    "\n",
    "    # --- 7. Extract 3D Mesh ---\n",
    "    print(\"\\n[RESULT] Extracting final mesh...\")\n",
    "    pred_mesh = extract_3d_from_nerf(\n",
    "        model,\n",
    "        resolution=MESH_RESOLUTION,\n",
    "        bound=MESH_BOUND,\n",
    "        sigma_scale=SIGMA_SCALE_EXTRACT, # Potentially different scale for extraction\n",
    "        device=DEVICE,\n",
    "        sigma_threshold=SIGMA_THRESHOLD_EXTRACT # Use threshold if defined\n",
    "    )\n",
    "\n",
    "    # --- 8. Evaluate (Chamfer Distance) & Visualize ---\n",
    "    if pred_mesh is not None and gt_points is not None:\n",
    "        print(\"[RESULT] Calculating Chamfer Distance...\")\n",
    "        pred_points = pred_mesh.sample(5000) # Sample points from predicted mesh\n",
    "        # Ensure gt_points is numpy array on CPU\n",
    "        if isinstance(gt_points, torch.Tensor):\n",
    "            gt_points_np = gt_points.cpu().numpy()\n",
    "        else:\n",
    "            gt_points_np = gt_points\n",
    "\n",
    "        cd = chamfer_distance(pred_points, gt_points_np)\n",
    "        print(f\"  --> Chamfer Distance (Pred Mesh vs. GT Points): {cd:.6f}\")\n",
    "\n",
    "        # Optional: Plot comparison if GT mesh also exists\n",
    "        if gt_mesh is not None:\n",
    "             plot_meshes(gt_mesh, pred_mesh, outpath=os.path.join(DEBUG_DIR,\"final_mesh_comparison.png\"))\n",
    "        else:\n",
    "             # Just save the predicted mesh render if no GT mesh\n",
    "             try:\n",
    "                 scene = trimesh.Scene(pred_mesh)\n",
    "                 scene.camera.elevation = np.radians(30) # Set view angle\n",
    "                 png = scene.save_image(resolution=(600, 600))\n",
    "                 with open(os.path.join(DEBUG_DIR,\"final_predicted_mesh.png\"), \"wb\") as f:\n",
    "                     f.write(png)\n",
    "                 print(f\"[PLOT] Saved predicted mesh render to {os.path.join(DEBUG_DIR,'final_predicted_mesh.png')}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"[PLOT] Failed to render predicted mesh: {e}\")\n",
    "\n",
    "    elif pred_mesh is None:\n",
    "        print(\"[RESULT] Mesh extraction failed. No evaluation possible.\")\n",
    "    else: # pred_mesh exists but gt_points is None\n",
    "        print(\"[RESULT] Predicted mesh extracted, but no GT points for comparison.\")\n",
    "        # Save predicted mesh render\n",
    "        try:\n",
    "            scene = trimesh.Scene(pred_mesh)\n",
    "            scene.camera.elevation = np.radians(30)\n",
    "            png = scene.save_image(resolution=(600, 600))\n",
    "            with open(os.path.join(DEBUG_DIR,\"final_predicted_mesh.png\"), \"wb\") as f:\n",
    "                 f.write(png)\n",
    "            print(f\"[PLOT] Saved predicted mesh render to {os.path.join(DEBUG_DIR,'final_predicted_mesh.png')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[PLOT] Failed to render predicted mesh: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n[DONE] Script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
